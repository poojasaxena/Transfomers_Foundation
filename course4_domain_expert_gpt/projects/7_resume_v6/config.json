{
  "model_config": {
    "d_model": 384,
    "n_heads": 12,
    "n_layers": 12,
    "max_position_embeddings": 512,
    "dropout": 0.1,
    "vocab_size": 6144
  },
  "training_config": {
    "batch_size": 8,
    "block_size": 512,
    "learning_rate": 5e-05,
    "train_steps": 12000,
    "num_epochs": 12,
    "eval_interval": 500,
    "eval_iters": 200,
    "warmup_steps": 0,
    "weight_decay": 0.0001,
    "early_stopping": {
      "enabled": true,
      "patience_evals": 10,
      "min_delta": 0.0
    },
    "resume_from": "/home/pooja-saxena/PoojaVault/Professional/Workbench/Models/llm/language_models/elephantdomain_gpt/gpt-bpe-v6/best.pt",
    "save": {
      "save_last": true,
      "save_best": true,
      "finalize_model_as": "best"
    }
  },
  "project_metadata": {
    "model_name": "gpt-bpe-v7",
    "model_save_path": "llm/language_models/elephantdomain_gpt/",
    "tokenizer_save_path": "llm/tokenizers/bpe-elephant/v4/tokenizer.json",
    "data_path": "llm/mixed_text/out",
    "data_file": "train.txt",
    "val_file": "val.txt",
    "test_file": "test.txt",
    "max_seq_length": 512,
    "max_new_tokens": 200
  },
  "tokenizer_config": {
    "type": "byte_bpe",
    "vocab_size": 6144,
    "min_freq": 2,
    "special_tokens": [
      "<pad>",
      "<unk>",
      "<bos>",
      "<eos>"
    ]
  },
  "inference_config": {
    "seed": 42,
    "temperature": 0.7,
    "top_k": 50,
    "avoid_eos_first_n": 10
  }
}

{
    "model_config": {
      "d_model": 384,
      "n_heads": 12,
      "n_layers": 12,
      "max_position_embeddings": 512,
      "dropout": 0.1,
      "vocab_size": 6144
    },
    "training_config": {
      "batch_size": 8,
      "block_size": 512,
      "learning_rate": 1e-4,
      "train_steps": 35000,
      "num_epochs": 12,
      "eval_interval": 500,
      "eval_iters": 200,
      "warmup_steps": 1000,
      "weight_decay": 1e-4,
      "early_stopping": {
            "enabled": true,
            "patience_evals": 8,
            "min_delta": 0.0
          },
      "save": {
             "save_last": true,
             "save_best": true,
             "finalize_model_as": "best"
      }
    },
    "project_metadata": {
      "model_name": "gpt-bpe-v6",
      "model_save_path": "llm/language_models/elephantdomain_gpt/",
      "tokenizer_save_path": "llm/tokenizers/bpe-elephant/v4/tokenizer.json",
      "data_path": "llm/mixed_text/out",
      "data_file": "train.txt", 
      "val_file": "val.txt",
      "test_file": "test.txt",
      "max_seq_length": 512,
      "max_new_tokens": 200
    },
    "tokenizer_config": {
      "type": "byte_bpe",
      "vocab_size": 6144,
      "min_freq": 2,
      "special_tokens": ["<pad>", "<unk>", "<bos>", "<eos>"]
    },
    "inference_config": {
        "seed": 42,                     
        "temperature": 0.7,
        "top_k": 50,
        "avoid_eos_first_n": 10
}

}

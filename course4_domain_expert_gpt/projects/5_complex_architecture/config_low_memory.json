{
    "model_config": {
      "d_model": 256,
      "n_heads": 8,
      "n_layers": 6,
      "max_position_embeddings": 256,
      "dropout": 0.15,
      "num_embeddings": 6144
    },
    "training_config": {
      "batch_size": 4,
      "block_size": 256,
      "learning_rate": 2e-4,
      "train_steps": 18000,
      "num_epochs": 8,
      "eval_interval": 200,
      "eval_iters": 100,
      "warmup_steps": 500,
      "weight_decay": 2e-4
    },
    "project_metadata": {
      "model_name": "gpt-bpe-v5",
      "model_save_path": "llm/language_models/elephantdomain_gpt/",
      "tokenizer_save_path": "llm/tokenizers/bpe-elephant/v4/tokenizer.json",
      "data_path": "llm/mixed_text/out",
      "data_file": "train.txt", 
      "val_file": "val.txt",
      "test_file": "test.txt",
      "max_seq_length": 256,
      "max_new_tokens": 150
    },
    "tokenizer_config": {
      "type": "byte_bpe",
      "vocab_size": 6144,
      "min_freq": 2,
      "special_tokens": ["<pad>", "<unk>", "<bos>", "<eos>"]
    }
}

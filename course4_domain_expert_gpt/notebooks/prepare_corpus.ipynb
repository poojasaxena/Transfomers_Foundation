{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c41f48-64da-4710-8314-1e971d404e55",
   "metadata": {},
   "source": [
    "## Remove Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec6719e-ea3d-40c0-966c-8b10e4a9041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e643b29-0127-4cdc-8cc0-557b4a0cc0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dedupe and filter corpus in  /home/pooja-saxena/PoojaVault/Professional/Workbench/Datasets/llm/mixed_text/out\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path(os.environ[\"GLOBAL_DATASETS_DIR\"]) / \"llm/mixed_text/out/\"\n",
    "print(\"Dedupe and filter corpus in \", DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da2e5517-902f-4241-9171-8c2e973abb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/pooja-saxena/PoojaVault/Professional/Workbench/Datasets/llm/mixed_text/out/elephant_human_90_10_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "p = DATA_ROOT / \"elephant_human_90_10_corpus.txt\"\n",
    "print(\"Reading\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c718a1ae-e77d-4bdf-8834-4265941c3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [l.rstrip() for l in p.open(encoding=\"utf-8\") if l.strip()]\n",
    "# dedupe (preserve order)\n",
    "seen = set()\n",
    "uniq = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a284163-1970-4f55-b6d7-c1d2beb87c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1522\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd59314b-c44f-4d0c-bda0-fe0f5f8430ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in lines:\n",
    "    if l in seen:\n",
    "        continue\n",
    "    seen.add(l)\n",
    "    uniq.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52e22443-2220-44b6-a556-2665229a7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra filters: remove lines that are too short or too long\n",
    "filtered = [l for l in uniq if 10 <= len(l) <= 500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e277b8c-273e-4213-9998-f0d6a127c0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig: 1522 Unique: 1482 Filtered: 1329\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# write\n",
    "out_path = DATA_ROOT / \"elephant_human_90_10_corpus.dedup.txt\"\n",
    "out_path.write_text(\"\\n\".join(filtered) + \"\\n\", encoding=\"utf-8\")\n",
    "print(\"Orig:\", len(lines), \"Unique:\", len(uniq), \"Filtered:\", len(filtered))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b22a45-a053-40fd-bf78-d272c6297f63",
   "metadata": {},
   "source": [
    "## Split Train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b3d2fa5-df34-4c6a-a3f0-e411c786c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_split_corpus(text_file: Path = src, seed: int = 42, splits=(0.98, 0.01, 0.01)):\n",
    "    lines = [l.rstrip() for l in text_file.open(encoding=\"utf-8\") if l.strip()]\n",
    "    random.seed(seed)\n",
    "    random.shuffle(lines)\n",
    "    n = len(lines)\n",
    "    n_val = max(1, int(n * splits[1]))\n",
    "    n_test = max(1, int(n * splits[2]))\n",
    "    n_train = n - n_val - n_test\n",
    "\n",
    "    train = lines[:n_train]\n",
    "    val = lines[n_train:n_train+n_val]\n",
    "    test = lines[n_train+n_val:]\n",
    "\n",
    "    (DATA_ROOT / \"train.txt\").write_text(\"\\n\".join(train)+\"\\n\", encoding=\"utf-8\")\n",
    "    (DATA_ROOT / \"val.txt\").write_text(\"\\n\".join(val)+\"\\n\", encoding=\"utf-8\")\n",
    "    (DATA_ROOT / \"test.txt\").write_text(\"\\n\".join(test)+\"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Counts: train\", len(train), \"val\", len(val), \"test\", len(test))\n",
    "    print(\"Chars train:\", sum(len(l) for l in train))\n",
    "\n",
    "    for name in [\"train.txt\",\"val.txt\",\"test.txt\"]:\n",
    "        p = DATA_ROOT / name\n",
    "        lines = [l.rstrip() for l in p.open(encoding=\"utf-8\") if l.strip()]\n",
    "        chars = sum(len(l) for l in lines)\n",
    "        words = sum(len(l.split()) for l in lines)\n",
    "        print(name, \"lines\", len(lines), \"chars\", chars, \"words\", words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76fc2360-2376-41a0-a95a-5fda4156e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(os.environ[\"GLOBAL_DATASETS_DIR\"]) / \"llm/mixed_text/out/\"\n",
    "src = DATA_ROOT / \"elephant_human_90_10_corpus.dedup.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fde67cd3-1566-46f3-ae13-5fad9f167707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: train 1303 val 13 test 13\n",
      "Chars train: 102113\n",
      "train.txt lines 1303 chars 102113 words 16281\n",
      "val.txt lines 13 chars 683 words 110\n",
      "test.txt lines 13 chars 943 words 149\n"
     ]
    }
   ],
   "source": [
    "smart_split_corpus(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb828d-4f1a-48e6-9d26-dbcf3b3a2232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3dbae30-fab1-4a88-881e-8e5aa0eac11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt lines 1303 chars 101362 words 16167\n",
      "val.txt lines 13 chars 1018 words 154\n",
      "test.txt lines 13 chars 1359 words 219\n"
     ]
    }
   ],
   "source": [
    "for name in [\"train.txt\",\"val.txt\",\"test.txt\"]:\n",
    "    p = DATA_ROOT / name\n",
    "    lines = [l.rstrip() for l in p.open(encoding=\"utf-8\") if l.strip()]\n",
    "    chars = sum(len(l) for l in lines)\n",
    "    words = sum(len(l.split()) for l in lines)\n",
    "    print(name, \"lines\", len(lines), \"chars\", chars, \"words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43248a-2703-4f64-a8c0-e86706ba7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-course)",
   "language": "python",
   "name": "llm-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "model_config": {
    "d_model": 16,
    "n_heads": 2,
    "n_layers": 2,
    "max_position_embeddings": 32,
    "dropout": 0.1
  },
  "training_config": {
    "batch_size": 64,
    "learning_rate": 0.003,
    "train_steps": 1200,
    "num_epochs": 10,
    "eval_interval": 100,
    "eval_iters": 200
  },
  "project_metadata": {
     "model_name": "greet-v1",
     "model_save_path": "llm/language_models/tiny_gpt",
     "data_path": "llm/greetings",
     "data_file": "greetings.txt",
     "max_seq_length": 32,
     "max_new_tokens": 20
  }
}
